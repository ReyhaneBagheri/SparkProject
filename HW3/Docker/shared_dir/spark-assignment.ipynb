{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3a64308",
   "metadata": {},
   "source": [
    "<div align=\"middle\">\n",
    "  <h1><b><i>تمرین سوم</i></b></h1>\n",
    " </div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e9fe0e",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "    \n",
    "   #  بخش اول:  هدوپ \n",
    "\n",
    "    ۱- به سوالات زیر پاسخ دهید\n",
    "    - مفهوم replication جیست\n",
    "    - مفهوم block در HDFS چیست و اگر بلاک‌ها را بسیار کوچک درنظر بگیریم چه مشکلی پیش می‌آید\n",
    "    \n",
    "</div>\n",
    "  \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c04ddc68",
   "metadata": {},
   "source": [
    "    # answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5182bf",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "   \n",
    "\n",
    "    ۲- در  این قسمت شما باید ابتدا دیتاست داده شده را از حالت فشرده در بیاورید و سپس فایل‌های درون آن را در کلاستر هدوپ در مسیر /homework3/dataset/ بارگزاری کنید \n",
    "    نکته: در این بخش دستورات زده شده خود را برای کار با hdfs  در ترمینال را در سلول زیر وارد نمایید\n",
    "    برای دسترسی به کامند 'hdfs dfs'  میتوانید وارد یکی از کانتینر‌های هدوپ شوید و دستور را اجرا کنید همچنین همه کانتینر‌ها دارای shared_dir\n",
    "    در روت خود هستند و این دایرکتوری در تمام کانتینر‌ها به اشتراک گذاشته شده است\n",
    "    برای چک کردن فایل‌ها در hdfs به \n",
    "    \n",
    "[HDFS webUI](http://localhost:9870/explorer.html#/)\n",
    "    \n",
    "    مراجعه کنید\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e537a25c",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e54482",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "    \n",
    "   #  بخش دوم:  اسپارک \n",
    "\n",
    "    ۱- به سوالات زیر پاسخ دهید\n",
    "    - مزیت اسپارک نسبت به مدل قدیمی map/reduce چیست؟\n",
    "    -  تفاوت action و transform در اسپارک چیست؟\n",
    "    \n",
    "</div>\n",
    "  \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e9ff8af",
   "metadata": {},
   "source": [
    "    # answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55619e4a",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "    \n",
    "\n",
    "    ۲- کد‌های خواسته شده در قسمت‌های پایینی را تکمیل کنید\n",
    "    (قسمت های ToDo )\n",
    "</div>\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf3a3a6",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "   \n",
    "##  اتصال به کلاستر اسپارک و هدوپ \n",
    "\n",
    "    در این قسمت از تمرین باید به عنوان درایور یک سسشن  به کلاستر اسپارک بسازیم.\n",
    " </div>\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bc58326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext \n",
    "from pyspark.sql import SparkSession,Row\n",
    "from pyspark.sql.functions import *\n",
    "import math\n",
    "import pandas , numpy\n",
    "import matplotlib\n",
    "import pprint\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "803466dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.20.30.100:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>homework3</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5704403520>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"homework3\").master(\"spark://spark-master:7077\").config(\"fs.defaultFS\",\"hdfs://namenode:9000/\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b42d9da",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "   \n",
    "#  خواندن داده \n",
    "\n",
    "    :در اسپارک ما ساختارهای مختلفی برای کار با داده و پخش شدن آن‌ها در شبکه داریم که به ۳ دسته تقسیم بندی میشوند \n",
    "+ RDD\n",
    "+ Dataset\n",
    "+ DataFrame\n",
    "    \n",
    "    \n",
    "     برای مطالعه بیشتر به لینک زیر مراجعه کنید:\n",
    "[rdd-vs-dataframe-vs-dataset](https://phoenixnap.com/kb/rdd-vs-dataframe-vs-dataset)\n",
    "\n",
    "    ما در درس با ساختار RDD آشنا شدیم حال در این تمرین میخواهیم با ساختار Dataframe آشنا شویم و به کمک آن دیتا را از روی HDFS بخوانیم و روی آن فایل‌ها پردازش انجام دهیم\n",
    " \n",
    " </div>\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6bc918",
   "metadata": {},
   "source": [
    "- [Spark Cluster Master UI](http://localhost:8080/)\n",
    "- [Application master UI (driver UI)](http://localhost:4040)\n",
    "- [Web Hdfs](http://localhost:9870/explorer.html#/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ad03c8",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    " .در این قسمت دیتاست را لود می‌کنیم \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476d0512",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "War = spark.read.json(f\"/homework3/dataset/War.json\")\n",
    "Weapon = spark.read.json(\"/homework3/dataset/Weapon.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742bf2d3",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "برای اینکه بتوانیم روی دیتای لود شده به وسیله تابع  spark.sql\n",
    "    کوئری‌های SQL بزنیم\n",
    "    باید دو دیتاست لود شده را به عنوان table\n",
    "    به spark \n",
    "    معرفی کنیم\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce77c45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "War.registerTempTable(\"War\")\n",
    "Weapon.registerTempTable(\"Weapon\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f11b6b",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "یک مثال ...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec5c8966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------+-------------+--------------+------------+\n",
      "|  DateOfWar|DurationOfWar|Location|MinorityStart|TargetMinority|      Weapon|\n",
      "+-----------+-------------+--------+-------------+--------------+------------+\n",
      "|73298-04-22|       1005.0|BIEN HOA|          Elf|           Orc|Mirkwood Bow|\n",
      "+-----------+-------------+--------+-------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM War where DurationOfWar=1005.0 limit 1;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09d05cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------+-------------+--------------+------------+\n",
      "|  DateOfWar|DurationOfWar|    Location|MinorityStart|TargetMinority|      Weapon|\n",
      "+-----------+-------------+------------+-------------+--------------+------------+\n",
      "|73361-06-05|       1005.0|TAN SON NHUT|          Elf|           Orc|Belthronding|\n",
      "+-----------+-------------+------------+-------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "War.select(\"*\").filter(col(\"DurationOfWar\")==1005.0).limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75cd88a",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    " ده رکورد آخر را نمایش دهید\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad2ef3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01038949",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "اسکیما یا ساختار دیتاست ها را نمایش دهید\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b16998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73d9c0e8",
   "metadata": {},
   "source": [
    "### Expected\n",
    "![image](expected_answers/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c495401",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "   \n",
    "#  اسپارک SQL \n",
    "\n",
    "    مهمترین قابلیت اسپارک این است که می‌تواند با خواندن فایل‌ها به صورت توزیع شده روی آن‌ها پردازش انجام دهد و این پردازش را برنامه ‌نویس میتواند با استفاده از دستورات SQL اعمال کند\n",
    "    در این بخش از شما انتظار می‌رود که به وسیله spark SQL  به اسپارک کوئری  بزنید . \n",
    "    \n",
    "    \n",
    " </div>\n",
    "\n",
    "[pyspark.sql.functions.col](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.col.html)\n",
    "\n",
    "[pyspark.sql.DataFrame.count](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.count.html)\n",
    "\n",
    "[pyspark.sql.DataFrame.filter](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.filter.html)\n",
    "\n",
    "[pyspark.sql.DataFrame.groupBy](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.groupBy.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c34aeac",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    " چه تعداد نبرد در کل این دوران‌ها انجام شده است؟\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eb020a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72628c2d",
   "metadata": {},
   "source": [
    "### Expected\n",
    "![image](expected_answers/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6d60ab",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "هر نژاد در چه تعدادی نبرد مشارکت داشته اند  به صورت مرتب شده نمایش دهبد؟\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ec8858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d977de3e",
   "metadata": {},
   "source": [
    "### Expected\n",
    "![image](expected_answers/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df744cf",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "   \n",
    "#  اسپارک toPandas \n",
    "\n",
    "    یکی ار قابلیت‌های اسپارک این است که می‌توان dataframe های آن را به \n",
    "    dataframe های pandas تبدیل کند و از توابع آن از جمله\n",
    "    توابع plot  آن برای رسم نمودار استفاده کرد.\n",
    "    در این قسمت از شما انتظار می‌رود نمودار تعداد جنگ‌ها بر اساس هر گونه را رسم کنید.\n",
    "    \n",
    " </div>\n",
    "  \n",
    "    \n",
    "\n",
    "  \n",
    "[pyspark.sql.DataFrame.toPandas](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.toPandas.html)\n",
    "\n",
    "[pyspark.pandas.DataFrame.plot](https://spark.apache.org/docs/3.2.1/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.plot.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a944ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b771d206",
   "metadata": {},
   "source": [
    "### Expected\n",
    "![image](expected_answers/plotlib.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ee0328",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "چه تعداد نبردهایی بر اساس زمان انجام شده بر اساس زمان مرتب کنید\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9baedbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d763296",
   "metadata": {},
   "source": [
    "### Expected\n",
    "![image](expected_answers/date_groupBy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e1daad",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "نژاد \"اورک\" توسط چه نژادی مورد حمله قرار گرفته است ؟\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12201bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f873db0f",
   "metadata": {},
   "source": [
    "### Expected\n",
    "![image](expected_answers/orc_target.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa7132b",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "   \n",
    "#  اسپارک Shuffle \n",
    "\n",
    "    Spark SQL shuffle مکانیزمی است برای توزیع مجدد یا پارتیشن بندی مجدد داده ها به طوری که داده ها به طور متفاوت در پارتیشن ها گروه بندی می شوند، بر اساس اندازه داده شما ممکن است نیاز باشد تعداد پارتیشن های RDD/DataFrame را با استفاده از اسپارک کاهش یا افزایش دهید.\n",
    "    برای مثال وقتی روی دو dataframe مختلف که روی شبکه توزیع شده اند\n",
    "    دستور join را میزنیم یک عملیات \n",
    "    shuffling انجام میشود\n",
    "    در این قسمت از شما انتظار می‌رود کوئری جوین زیر را نوشته و اجرا کنید همچنین به  صفحه \n",
    "    application master ui\n",
    "    مراجعه کنید و نحوه shuffleing را گزارش کنید . \n",
    "     و همچنین توضیح دهید DAG scheduler  در اسپارک چیست ؟\n",
    " </div>\n",
    "  \n",
    "    \n",
    "\n",
    "  \n",
    "[shuffling in standalone cluster](https://medium.com/@rachit1arora/apache-spark-shuffle-service-there-are-more-than-one-options-c1a8e098230e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cc72d6",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"auto\">\n",
    "    \n",
    "    بیشترین سلاحی که در این جنگ ها استفاده شده کدام سلاح  بوده است؟.\n",
    "    جزییات این سلاح را از فایل مشخصات سلاح (Weapon) می‌توانید بدست آورید.\n",
    "    فایل مربوط به سلاح ها را بازخوانی کرده و برای نمایش بین دو فایل از join استفاده شود.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3168365c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8d77f32",
   "metadata": {},
   "source": [
    "### Expected\n",
    "![image](expected_answers/most_used_weapons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d810f555",
   "metadata": {},
   "source": [
    "     # ToDo\n",
    "     # Refere to Application master UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfce0ec",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "   \n",
    "# تابع تعریف شده توسط کاربر(UDF)\n",
    "\n",
    "    یکی از مزایای اسپارک این است که نه تنها برای ما یک زبان SQL فراهم کرده که روی چندین سرور به صورت همزمان پردازش را انجام دهد بلکه \n",
    "    میتوان به زبان‌های مختلف توابعی تعریف کرد که روی  همه executor ها اجرا شود\n",
    "    در این بخش از شما انتظار می‌رود که با نوشتن یک UDF به زبان پایتونی \n",
    "    توضیحات هر اسلحه را به 3 کلمه اول آن کوتاه کنید و بقیه کلمات را حذف کنید و سوتونی به نام short_description را به داده های \n",
    "    Weapon اضافه کنید\n",
    "    \n",
    "    \n",
    " </div>\n",
    " \n",
    "[pyspark.sql.functions.udf](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html)\n",
    "\n",
    "[pyspark.sql.DataFrame.withColumn](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.withColumn.html)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf66e165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1276e859",
   "metadata": {},
   "source": [
    "### Expected\n",
    "![image](expected_answers/shortener_udf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e692106b",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"right\" dir=\"auto\">\n",
    "   \n",
    "# نوشتن داده و پارتیشنینگ\n",
    "    زمانی که میخواهیم داده های حجیم را به صورت فایل هایی ذخیره کنیم  نمیتوانیم همه داده را در یک فایل بزرگ ذخیره کنیم به چند دلیل :\n",
    "-  ذخیره سازی یک فایل بزرگ باعث ؛تنها نقطه شکست میشود؛ و با حذف آن کل داده از دست میرود\n",
    "- جستجو در این یک فایل بزرگ که مرتب شده نیست دشوار و عملی نیست\n",
    "- آپدیت کردن سخت تر میشود\n",
    "پس تا حدودی حل این میشکل از راه حل های زیر استفاده میکنیم\n",
    "# پارتیشنینگ: \n",
    "-     بر اساس یک فیلد داده هارا دسته بندی میکنیم و در دایرکتوری های مختلف میریزیم این کار را اسپارک برای ما انجام میدهد\n",
    "\n",
    "    \n",
    " </div>\n",
    "  \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08998af6",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "\n",
    "    در این بخش شما باید یک دیتاست کامل بسازید بدین شکل که دیتاست War و Weapon\n",
    "     را با یکدیگر جوین کنید سپس همه آن رکورد‌هایی که  در توضیحات اسلحه آنها کلمه sword \n",
    "    نیامده است را فیلتر کرده و برا اساس گونه شروع کننده جنگ (MinorityStart) پارتیشن کنید\n",
    "    و روی HDFS\n",
    "      در مسیر /homework3/<student_number>/output/war_without_sword بنویسید\n",
    "    و ۲۰ رکورد اول آن را نمایش دهید\n",
    "    توجه کنید فایل‌های خروجی باید از نوع JSON باشند\n",
    "    \n",
    "    \n",
    "</div>\n",
    "\n",
    "[pyspark.sql.DataFrameWriter.partitionBy](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.partitionBy.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c49791",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f910e168",
   "metadata": {},
   "source": [
    "### Expected\n",
    "![war without sword](expected_answers/war_without_sword.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd43ed9",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "    \n",
    "  ### گزارش HDFS\n",
    "    با مراجعه به HDFS UI\n",
    "    خروجی پارتیشن شده مسیر بالا را مشاهده میکنید\n",
    "     -تعداد دایرکتوری‌های ایجاد شده چندتاست و دلیل آن چیست؟ \n",
    "    -در هر دایرکتوری ایجاد شده چند فایل JSON میبینید و دلیل تعدد این فایل‌ها چیست؟\n",
    "    - چگونه میتوان از اسپارک خواست تا از تعدد این فایل‌ها جلوگیری کند و یک فایل در این مسیر‌ها بریزد؟\n",
    "    - coalesce و repartition  در اسپارک چیستند و کاربر اصلی آنها چیست؟\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a025d2c6",
   "metadata": {},
   "source": [
    "    # ToDo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bb7441",
   "metadata": {},
   "source": [
    "<div align=\"right\" dir=\"rtl\">\n",
    "\n",
    "   ## coalesce\n",
    "    همان تسک بالا را انجام دهید  ولی  سعی کنید در هر دایرکتوری پارتیشن شده تنها یک فایل JSON ریخته شود.\n",
    "    نکته:  در مسیر جدید زیر آن را بنویسید.\n",
    "    /homework3/<student_number>/output/war_without_sword_v2/\n",
    "    \n",
    "    \n",
    "</div>\n",
    "\n",
    "[pyspark.sql.DataFrame.coalesce](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.coalesce.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041f6352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcfd90d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
